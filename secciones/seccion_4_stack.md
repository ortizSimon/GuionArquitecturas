# Secci√≥n 4 ‚Äî Stack tecnol√≥gico (Tabla 2)

> **Estado:** üîÑ En construcci√≥n  
> **Trazabilidad:** Componentes (Secci√≥n 3) ‚Üí RNF (Secci√≥n 2) ‚Üí Justificaci√≥n de elecci√≥n tecnol√≥gica

---

## Criterios de selecci√≥n

Las tecnolog√≠as se seleccionan con base en:
1. Restricciones expl√≠citas del enunciado (multiplataforma, seguridad estatal, integraciones externas).
2. Alineaci√≥n con los RNF identificados (Secci√≥n 2).
3. Madurez en el ecosistema financiero / bancario colombiano.
4. [POR DEFINIR] Restricciones adicionales del equipo o del curso.

---

## 4. Stack tecnol√≥gico (Tabla 2)

La siguiente tabla mapea cada capa del sistema a su tecnolog√≠a recomendada. **Modelo de despliegue: h√≠brido (on-premise Colombia + AWS).** La Superintendencia Financiera confirm√≥ que el alojamiento de datos regulados en Brasil (`sa-east-1`) no es aceptable; por lo tanto, los datos sensibles (autenticaci√≥n, PII de usuarios, transacciones financieras e hist√≥rico de auditor√≠a) residen en un **datacenter on-premise en Colombia**. El c√≥mputo de D1 (IAM) y D8 (Event Ingester / Report Generator) se co-localiza on-premise con sus bases de datos. Los servicios de D2, D3, D4, D5, D6 y D7 se ejecutan en **Amazon EKS** (`sa-east-1`); D2 y D4 conectan a sus bases de datos on-premise v√≠a **AWS Direct Connect** (2√ó 1 Gbps, ~15‚Äì25 ms). Los servicios administrados de AWS (MSK, ElastiCache Redis, CloudFront, OpenSearch, S3, observabilidad) se mantienen en `sa-east-1` para datos ef√≠meros y operacionales. `us-east-1` act√∫a como regi√≥n de DR.

### 4.1 Presentaci√≥n y acceso

| Capa / Componente | Tecnolog√≠a recomendada | Alternativa | Justificaci√≥n y trade-offs | RNF vinculado |
|---|---|---|---|---|
| **Web SPA + BFF** | React 18 + Next.js 14 desplegado en **AWS Amplify + CloudFront** | Angular 17 + S3/CloudFront | Amplify automatiza el pipeline de despliegue del frontend; CloudFront distribuye desde edge locations para < 2 s en Colombia. Next.js provee el patr√≥n BFF con API Routes. Angular es alternativa si el equipo ya lo domina. | Usabilidad, Tiempo de respuesta < 2 s |
| **App M√≥vil / Tablet** | Flutter 3 (Dart) + **AWS Amplify SDK** | React Native + Amplify | Un solo codebase para iOS, Android y tablet ‚â• 6"; compilaci√≥n nativa sin WebView; Amplify SDK integra llamadas API y notificaciones push; autenticaci√≥n v√≠a Keycloak (OIDC) a trav√©s de la librer√≠a `flutter_appauth` compatible con OAuth2/OpenID Connect. React Native tiene mayor ecosistema JS pero m√°s overhead en el puente nativo. | Usabilidad, Compatibilidad multiplataforma |
| **API Gateway** | **Amazon API Gateway (HTTP API)** | Kong Gateway (self-hosted en EKS) | Fully managed: throttling, autorizaci√≥n JWT (validaci√≥n de tokens emitidos por Keycloak via JWT authorizer), WAF integrado y trazabilidad con X-Ray incluidos; escala autom√°tica a millones de peticiones. Kong da mayor control sobre plugins personalizados pero requiere operar el gateway y un cluster de K8s adicional. | Seguridad, Disponibilidad, Escalabilidad |

---

### 4.2 Mensajer√≠a y comunicaci√≥n as√≠ncrona

| Capa / Componente | Tecnolog√≠a recomendada | Alternativa | Justificaci√≥n y trade-offs | RNF vinculado |
|---|---|---|---|---|
| **Message Broker** | **Amazon MSK** (Managed Streaming for Apache Kafka) | Confluent Cloud | MSK elimina la gesti√≥n del plano de control de Kafka; integraci√≥n nativa con IAM, VPC, CloudWatch y MSK Connect para sink/source connectors hacia otros servicios AWS. Confluent a√±ade Schema Registry y mayor tooling de monitoreo pero con costo adicional y dependencia de tercero. | Escalabilidad, Fiabilidad, Consistencia eventual |
| **Service Mesh (mTLS inter-servicio)** | **Istio 1.x sobre EKS** | AWS App Mesh | Istio sobre EKS provee mTLS autom√°tico entre pods, circuit breaker, retry policies y m√©tricas de tr√°fico; integraci√≥n con OpenTelemetry para trazas. AWS App Mesh es m√°s ligero y nativo en AWS pero con menor madurez de comunidad y roadmap incierto. | Seguridad (canales inter-servicio), Disponibilidad |

---

### 4.3 Microservicios ‚Äî Runtime

| Capa / Componente | Tecnolog√≠a recomendada | Alternativa | Justificaci√≥n y trade-offs | RNF vinculado |
|---|---|---|---|---|
| **Dominios financieros ‚Äî D3, D4, D7** | Java 21 + Spring Boot 3 en **Amazon EKS** (node groups EC2) | Kotlin + Spring Boot en EKS | ACID nativo con Spring Data JPA; Spring Batch para cargas masivas de D3; librer√≠as Saga maduras (Eventuate Tram, Axon Framework); EC2 node groups permiten elegir instancias optimizadas para c√≥mputo intensivo en picos de n√≥mina. **D4 conecta a PostgreSQL on-premise v√≠a Direct Connect** (~15‚Äì25 ms mitigados por cach√© Redis en AWS); D3 y D7 usan Aurora PostgreSQL en AWS. Kotlin reduce verbosidad pero a√±ade curva de adopci√≥n al equipo. | Consistencia, Fiabilidad, Trazabilidad |
| **Dominios API / integraci√≥n ‚Äî D2, D5, D6** | Node.js 20 + NestJS en **Amazon EKS + Fargate** | Go (Golang) en EKS | Alta concurrencia I/O para servicios orientados a API; Fargate elimina la gesti√≥n de nodos para pods de escala variable; TypeScript garantiza tipado estricto. **D2 conecta a PostgreSQL on-premise v√≠a Direct Connect** (~15‚Äì25 ms mitigados por cach√© Redis de saldo); D5 y D6 usan Aurora PostgreSQL en AWS. Go ofrece menor latencia pero con ecosistema de librer√≠as financieras m√°s reducido. | Tiempo de respuesta < 2 s, Extensibilidad |
| **Identidad y acceso ‚Äî D1** | **Keycloak 24** (Java) + **NestJS** thin API, desplegados en **cluster K8s on-premise (Colombia)** | Amazon Cognito + Lambda | Co-localizado con su PostgreSQL on-premise para eliminar latencia inter-DC en operaciones de autenticaci√≥n (P95 login < 1.5 s). Keycloak maneja autenticaci√≥n, MFA, OAuth2/OIDC y gesti√≥n de sesiones; el servicio NestJS publica `UserAuthenticated` / `SuspiciousLoginDetected` a MSK v√≠a Direct Connect y gestiona pol√≠ticas RBAC custom. Los datos de autenticaci√≥n nunca salen del territorio colombiano. | Seguridad, Disponibilidad, Cumplimiento normativo |
| **Stream processing ‚Äî Fraude (D8)** | **Amazon Managed Service for Apache Flink** | Kafka Streams en EKS | Flink administrado sobre MSK: procesamiento stateful con ventanas de tiempo para detecci√≥n de patrones sospechosos; escala autom√°tica sin gesti√≥n de cluster; m√©tricas nativas en CloudWatch. Kafka Streams es m√°s simple pero con menor expresividad para CEP complejo. | Seguridad, Trazabilidad en tiempo real |
| **Event Ingester (D8)** | Java 21 + Spring Boot 3 en **cluster K8s on-premise (Colombia)** | Node.js + NestJS | Co-localizado con Apache Cassandra on-premise: consume eventos de D4/D5/D7 desde MSK v√≠a Direct Connect, los persiste en Cassandra (append-only) sin cruce inter-DC en la escritura. Reenv√≠a a OpenSearch (AWS) v√≠a Kinesis Data Firehose a trav√©s de Direct Connect para indexaci√≥n full-text. Java comparte modelos de dominio con Flink. | Trazabilidad, Integridad, Cumplimiento normativo |
| **Fraud List Manager (D8)** | Java 21 + Spring Boot 3 en **Amazon EKS** (Fargate) | Incluido en Event Ingester | Separado del Event Ingester para mantenerlo en AWS junto a Flink: consume `SuspiciousPatternDetected` desde MSK, actualiza listas antifraude y propaga a Redis (D4) directamente sin cruce de Direct Connect. Publica `FraudListUpdated` a MSK. | Seguridad, Trazabilidad en tiempo real |
| **Report Generator (D8)** | **Python 3.12** en **cluster K8s on-premise (Colombia)** (ejecuci√≥n bajo demanda) | Java + Spring Batch | Co-localizado con Cassandra on-premise: lee directamente del audit store sin cruce inter-DC; genera extractos trimestrales y reportes semestrales (PDF/CSV para bancos, XML para Superintendencia); sube reportes a S3 (AWS) v√≠a Direct Connect con cifrado SSE-KMS. Python ofrece ecosistema maduro para generaci√≥n de documentos (ReportLab, openpyxl, lxml). | Cumplimiento normativo, Trazabilidad |
| **Dashboard API (D8)** | Node.js 20 + NestJS en **Amazon EKS + Fargate** | Go (Golang) | API REST protegida por D1 que expone consultas sobre OpenSearch (b√∫squeda por correlation_id, alertas de fraude, estado de lotes). Comparte stack con D2/D5/D6 para consistencia tecnol√≥gica. | Observabilidad, Trazabilidad |

---

### 4.4 Almacenamiento

| Capa / Componente | Tecnolog√≠a recomendada | Alternativa | Justificaci√≥n y trade-offs | RNF vinculado |
|---|---|---|---|---|
| **BD relacional on-premise ‚Äî D1, D2, D4** | **PostgreSQL 16 self-managed on-premise (Colombia)** con Patroni (HA autom√°tico) + pgBackRest (backups) | Amazon RDS for PostgreSQL v√≠a VPN | Datos regulados (PII de ~25M usuarios, transacciones financieras, credenciales de autenticaci√≥n) permanecen en territorio colombiano. Patroni gestiona failover autom√°tico entre nodos primario/r√©plica en el datacenter. Streaming replication as√≠ncrona a r√©plica de lectura en AWS (`sa-east-1`) como DR. Cifrado por columna con pgcrypto + claves en HashiCorp Vault. | Cumplimiento normativo, Consistencia, Integridad |
| **BD relacional AWS ‚Äî D3, D5, D7** | **Amazon Aurora PostgreSQL** (Multi-AZ, Serverless v2) | Amazon RDS for PostgreSQL Multi-AZ | Datos de menor sensibilidad regulatoria (empresas aliadas, billetera, pagos masivos): Aurora ofrece hasta 5√ó el throughput de PostgreSQL est√°ndar; failover autom√°tico < 30 s; r√©plicas de lectura para CQRS; cifrado en reposo con KMS integrado. | Consistencia, Integridad, Disponibilidad 24/7 |
| **Cach√© distribuida ‚Äî D1, D2, D4, D5, D7** | **Amazon ElastiCache for Redis** (cluster mode, Multi-AZ) | ElastiCache Serverless | Lecturas sub-milisegundo para m√∫ltiples casos de uso: proxy de saldo en tiempo real (D2, TTL corto), listas antifraude y registro de bancos filiales (D4, TTL 60 s y 5 min respectivamente), idempotencia de operaciones (D5, TTL 24 h), contadores de intentos de login (D1), validaci√≥n de idempotencia de pagos de n√≥mina (D7, TTL 24 h). Replicaci√≥n Multi-AZ con failover autom√°tico; integraci√≥n VPC nativa. Serverless simplifica la operaci√≥n pero introduce latencia en cold-start inaceptable para el SLA de < 2 s. | Tiempo de respuesta < 2 s, Escalabilidad, Seguridad |
| **Event Sourcing / hist√≥rico inmutable on-premise ‚Äî D8** | **Apache Cassandra 4.1 self-managed on-premise (Colombia)** con replicaci√≥n multi-rack (RF=3) | Amazon Keyspaces (serverless en AWS) | El audit log completo con firmas digitales y hash chains permanece en territorio colombiano (exigencia de la Superfinanciera). Cassandra provee escrituras append-only, CQL nativo y escalabilidad lineal. TTL deshabilitado (retenci√≥n ‚â• 5 a√±os). Cifrado en reposo con Transparent Data Encryption + claves en HashiCorp Vault. Keyspaces es alternativa serverless si la regulaci√≥n cambiara, pero actualmente no cumple el requisito de soberan√≠a. | Cumplimiento normativo, Integridad, Trazabilidad |
| **B√∫squeda y reportes ‚Äî D8** | **Amazon OpenSearch Service** (Multi-AZ) | OpenSearch self-managed en EKS | Managed: actualizaciones autom√°ticas, Multi-AZ, indexaci√≥n full-text sobre el audit log; OpenSearch Dashboards para reportes de cumplimiento regulatorio; ingesta v√≠a Amazon Kinesis Firehose desde CloudWatch Logs. Self-managed da m√°s control pero aumenta carga operacional. | Trazabilidad, Cumplimiento normativo |
| **Almacenamiento de reportes regulatorios ‚Äî D8** | **Amazon S3** (SSE-KMS, versionado habilitado) | Azure Blob Storage | Durabilidad 99.999999999% (11 nines); cifrado con KMS para extractos trimestrales y reportes semestrales; versionado para mantener hist√≥rico de env√≠os; pol√≠ticas de lifecycle (S3 Intelligent-Tiering ‚Üí Glacier) para optimizar costos de retenci√≥n a largo plazo (>5 a√±os). Azure Blob Storage es alternativa multi-cloud pero sin integraci√≥n nativa con el ecosistema AWS. | Cumplimiento normativo, Seguridad, Trazabilidad |

---

### 4.5 Seguridad e identidad

| Capa / Componente | Tecnolog√≠a recomendada | Alternativa | Justificaci√≥n y trade-offs | RNF vinculado |
|---|---|---|---|---|
| **Identity Provider ‚Äî D1** | **Keycloak 24 self-hosted en K8s on-premise (Colombia)** | Amazon Cognito | Keycloak desplegado en el cluster K8s on-premise garantiza que los datos de autenticaci√≥n (hashes, MFA, sesiones de ~25M usuarios) nunca salen del territorio colombiano. MFA, RBAC avanzado, OAuth2/OIDC y SSO nativos; amplio uso en sector financiero LATAM. Cognito es fully managed pero los datos residen en AWS, lo cual fue rechazado por la Superfinanciera. | Seguridad, Cumplimiento normativo |
| **Gesti√≥n de secretos y cifrado ‚Äî modelo dual** | **AWS Secrets Manager + AWS KMS** (servicios en AWS: D3, D5, D6, D7, Redis, MSK) + **HashiCorp Vault on-premise** (servicios on-premise: D1, D2-BD, D4-BD, D8) | Solo KMS / Solo Vault | Modelo dual: Secrets Manager rota autom√°ticamente credenciales de Aurora (D3/D5/D7), MSK y APIs de terceros; KMS cifra datos en reposo en AWS. Vault on-premise gestiona secretos y cifrado de las BDs on-premise (D1, D2, D4, D8): Transit engine para cifrado por columna (pgcrypto), PKI engine para certificados mTLS del cluster on-premise, rotaci√≥n autom√°tica de credenciales PostgreSQL y Cassandra. Vault se sincroniza con Secrets Manager v√≠a Vault AWS Secrets Engine para credenciales cross-environment. | Seguridad (cifrado en reposo y tr√°nsito, firmado) |
| **Detecci√≥n de intrusiones (IDS)** | **Amazon GuardDuty + AWS Security Hub + AWS WAF** + **Falco** en EKS y K8s on-premise | Wazuh SIEM | GuardDuty analiza VPC Flow Logs, CloudTrail y DNS para detectar amenazas en AWS; Security Hub centraliza hallazgos; WAF protege el API Gateway de ataques OWASP Top 10. Falco se despliega en ambos clusters (EKS y K8s on-premise) para detecci√≥n a nivel de contenedor/kernel; los hallazgos on-premise se env√≠an a Security Hub v√≠a Direct Connect. Wazuh a√±ade correlaci√≥n SIEM m√°s avanzada pero con mayor carga operacional. | Seguridad (IDS/IPS), Trazabilidad |

---

### 4.6 Infraestructura y orquestaci√≥n

| Capa / Componente | Tecnolog√≠a recomendada | Alternativa | Justificaci√≥n y trade-offs | RNF vinculado |
|---|---|---|---|---|
| **Orquestaci√≥n de contenedores ‚Äî AWS** | **Amazon EKS** (plano de control administrado) + **AWS Fargate** para pods variables + **EC2 node groups** para pods de carga fija | EKS con solo EC2 Auto Scaling | EKS gestiona el plano de control K8s sin intervenci√≥n manual; Fargate para servicios de integraci√≥n y API (D2, D5, D6, D8 Dashboard API, D8 Fraud List Manager); EC2 para D3/D4/D7 y D8 Flink donde se necesita control de instancia. | Disponibilidad 24/7, Escalabilidad |
| **Orquestaci√≥n de contenedores ‚Äî On-premise (Colombia)** | **Kubernetes 1.29** (kubeadm) + **MetalLB** (balanceo L4) + **Longhorn** (almacenamiento persistente) | Rancher / RKE2 | Cluster K8s on-premise dedicado para D1 (Keycloak + NestJS API) y D8 (Event Ingester + Report Generator). kubeadm provee control total sobre el plano de control sin dependencia de proveedor; MetalLB asigna IPs del rango del datacenter; Longhorn provee vol√∫menes replicados para PostgreSQL y Cassandra. Rancher/RKE2 simplifica la gesti√≥n pero a√±ade dependencia de SUSE. M√≠nimo 3 nodos master + 3 nodos worker. | Cumplimiento normativo, Disponibilidad |
| **Conectividad h√≠brida** | **AWS Direct Connect** dedicado (2√ó 1 Gbps, rutas f√≠sicas distintas) + **VPN Site-to-Site** (IPSec overlay) | VPN sobre internet p√∫blico | Direct Connect provee latencia predecible (~15‚Äì25 ms Colombia ‚Üí S√£o Paulo) y ancho de banda garantizado para tr√°fico entre servicios AWS y BDs on-premise (D2‚ÜíPG, D4‚ÜíPG, Event Ingester‚ÜíMSK). Dos conexiones por rutas distintas garantizan resiliencia ante corte de fibra. VPN IPSec a√±ade cifrado en tr√°nsito adicional al TLS de la aplicaci√≥n. VPN sobre internet es m√°s econ√≥mica pero con latencia variable e inaceptable para SLA < 2 s. | Disponibilidad, Seguridad, Tiempo de respuesta < 2 s |
| **Infraestructura como C√≥digo** | **Terraform** (AWS provider + Kubernetes provider + Vault provider, state en S3 + DynamoDB locking) | AWS CDK + Ansible | Declarativo y multi-entorno: gestiona infraestructura AWS (EKS, MSK, Aurora, Redis) y on-premise (K8s manifests, Vault config, PostgreSQL/Cassandra provisioning) desde un solo repositorio. State remoto en S3 con locking en DynamoDB; entornos reproducibles para auditor√≠as de cumplimiento. CDK es m√°s acoplado a AWS y no gestiona on-premise nativamente. | Evoluci√≥n del sistema, Cumplimiento |

---

### 4.7 Observabilidad

| Capa / Componente | Tecnolog√≠a recomendada | Alternativa | Justificaci√≥n y trade-offs | RNF vinculado |
|---|---|---|---|---|
| **M√©tricas y alertas** | **Amazon CloudWatch** + **Amazon Managed Grafana** | Prometheus self-managed + Grafana | CloudWatch recolecta m√©tricas de todos los servicios AWS de forma nativa (EKS, Aurora, MSK, API Gateway); Managed Grafana conecta CloudWatch como datasource para dashboards operacionales. Prometheus self-managed ofrece mayor flexibilidad en PromQL pero requiere operar el scraper y el almacenamiento. | Disponibilidad, Tiempo de respuesta < 2 s |
| **Logs centralizados / Audit Log** | **Amazon CloudWatch Logs + Amazon OpenSearch Service** (ingesta v√≠a Kinesis Data Firehose) | ELK Stack self-managed en EKS | CloudWatch Logs centraliza logs de EKS, Aurora, API Gateway y Lambda; Firehose enruta hacia OpenSearch para b√∫squeda full-text del audit log; retenci√≥n y cifrado con KMS configurables por pol√≠tica regulatoria. ELK self-managed tiene mayor personalizaci√≥n pero aumenta la carga operacional significativamente. | Trazabilidad, Cumplimiento normativo |
| **Trazas distribuidas** | **AWS X-Ray** (instrumentaci√≥n via OpenTelemetry SDK) | Jaeger en EKS | X-Ray integra nativamente con API Gateway, EKS y todos los servicios AWS; Service Map visual identifica latencia entre microservicios para el SLA < 2 s; sin infraestructura adicional que operar. Jaeger ofrece sampling adaptativo m√°s fino pero requiere despliegue y almacenamiento propio. | Tiempo de respuesta < 2 s, Disponibilidad |
| **Bridge de observabilidad on-premise ‚Üí AWS** | **OpenTelemetry Collector on-premise** + **Prometheus node-exporter** con remote-write a **Amazon Managed Prometheus (AMP)** | Prometheus + Grafana self-managed on-premise | OTel Collector desplegado en K8s on-premise exporta m√©tricas, logs y trazas de D1/D8 a CloudWatch, X-Ray y AMP v√≠a Direct Connect, consolidando la observabilidad en los mismos dashboards de Managed Grafana. Prometheus node-exporter monitorea salud de nodos f√≠sicos (CPU, disco, red). La alternativa self-managed on-premise requiere operar un stack de observabilidad paralelo con mayor carga operacional. | Observabilidad, Disponibilidad |

---

### 4.8 CI/CD

| Capa / Componente | Tecnolog√≠a recomendada | Alternativa | Justificaci√≥n y trade-offs | RNF vinculado |
|---|---|---|---|---|
| **Pipelines CI/CD** | **AWS CodePipeline + AWS CodeBuild + Amazon ECR** | GitLab CI/CD con runners en EKS | Stack fully managed: CodeBuild construye y testea im√°genes de contenedores, ECR las almacena, CodePipeline orquesta stages con aprobaciones manuales antes de producci√≥n; integraci√≥n nativa con EKS y Terraform. GitLab CI/CD tiene mayor madurez para flujos complejos de revisi√≥n y pipelines multi-etapa, preferible si el repositorio ya est√° en GitLab. | Evoluci√≥n del sistema, Disponibilidad |

---

### 4.9 Detalle de stack ‚Äî D3: Empresas y Empleados

| Componente | Tecnolog√≠a | Alternativa | Por qu√© se eligi√≥ | RNF vinculado |
|---|---|---|---|---|
| **Carga masiva de empresas y empleados** | **Spring Batch** en EKS (mismo runtime Java/Spring Boot del servicio) | AWS Glue (ETL serverless) | Procesamiento en chunks de 500 registros con commit transaccional por chunk, reintentos configurables e idempotencia nativa (`upsert` por `external_emp_id + company_id`). Reutiliza el runtime del servicio sin infraestructura adicional. AWS Glue simplifica el ETL pero no ofrece el nivel de control transaccional requerido para garantizar idempotencia a nivel de registro. | RNF-D3-02 |
| **Cifrado de campos sensibles de empresa** | **AWS KMS** (cifrado por columna en Aurora para `tax_id` y `auth_config`) | Cifrado solo a nivel de disco | `tax_id` (NIT de la empresa) y `auth_config` (credenciales del API de la empresa aliada) se cifran por columna: incluso con acceso al disco, los campos cr√≠ticos requieren la clave KMS para descifrarse. Garantiza minimizaci√≥n de PII sin alterar el esquema relacional. | RNF-D3-03 |
| **Circuit Breaker por empresa aliada** | **Resilience4j** (instancia independiente por empresa, integrado en Spring Boot) | Istio (nivel de malla de red) | Cada empresa aliada tiene su propio circuito con umbral de error y tiempo de apertura configurables. Ante ca√≠da del API de empresa X, el circuito de X se abre y su lote queda en `PAUSED_API_ERROR` sin afectar a las otras 14 empresas. Istio act√∫a a nivel de red con menor granularidad por empresa aliada. | RNF-D3-01 |

---

### 4.10 Detalle de stack ‚Äî D4: Transferencias y Transacciones

| Componente | Tecnolog√≠a | Alternativa | Por qu√© se eligi√≥ | RNF vinculado |
|---|---|---|---|---|
| **Motor de saga y compensaci√≥n** | **Axon Framework / Eventuate Tram** (librer√≠a sobre Spring Boot) | Temporal.io (workflow engine) | Gesti√≥n expl√≠cita de estados de saga (`transfer_saga_state`) y compensaci√≥n autom√°tica integrada con Spring Boot y Amazon MSK (v√≠a Direct Connect). El Outbox Pattern garantiza que cada mutaci√≥n de estado y su evento Kafka se persistan en la misma transacci√≥n ACID contra **PostgreSQL on-premise**. Temporal a√±ade mayor expresividad pero requiere operar su propio cluster. | RNF-D4-01 |
| **Cach√© de listas antifraude** | **Amazon ElastiCache for Redis** (cluster mode, Multi-AZ, TTL 60 s) | ElastiCache Serverless | Latencia sub-milisegundo para consultar listas blanca/gris/negra, garantizando P99 < 200 ms sin impactar el SLA de 2 s. TTL de 60 s asegura propagaci√≥n r√°pida de actualizaciones desde D8. ElastiCache Serverless introduce latencia de cold-start incompatible con ese P99. | RNF-D4-05 |
| **Cach√© del registro de bancos filiales** | **Amazon ElastiCache for Redis** (TTL 5 min ‚Äî registro de bancos filiales para LiquidationRouter) | Consulta s√≠ncrona a D2 por cada transferencia | El LiquidationRouter necesita determinar en < 50 ms si el banco destino es filial o no. Cachear este registro con TTL de 5 min elimina una llamada s√≠ncrona a D2 del camino cr√≠tico sin riesgo de inconsistencia, dado que el conjunto de bancos filiales cambia con muy poca frecuencia. | RNF-D4-02 |
| **Escalado horizontal en picos de n√≥mina** | **Amazon EKS + HPA** con m√©tricas personalizadas v√≠a **CloudWatch Adapter** | KEDA (Kubernetes Event-Driven Autoscaling) | HPA escala los pods de D4 al detectar latencia de cola > 1.5 s o CPU > 70%; EC2 node groups permiten elegir instancias de mayor capacidad en los d√≠as de pago masivo (14‚Äì16 y 29‚Äì31). KEDA a√±ade mayor granularidad por tama√±o de cola Kafka pero con mayor complejidad operacional. | RNF-D4-01, RNF-D4-02 |

---

### 4.11 Detalle de stack ‚Äî D5: Billetera Digital

| Componente | Tecnolog√≠a | Alternativa | Por qu√© se eligi√≥ | RNF vinculado |
|---|---|---|---|---|
| **Base de datos del ledger** | **Aurora PostgreSQL** (tabla `wallet_entries`, solo se insertan registros, nunca se modifican) | PostgreSQL propio en contenedor | Aurora garantiza alta disponibilidad y recuperaci√≥n autom√°tica ante fallos. Al no permitir modificar registros, el historial de movimientos nunca se puede alterar. | RNF-D5-01 |
| **Saga coreografiada (consumidor de eventos)** | **NestJS** con consumidores Kafka (mismo lenguaje que el resto de D5) | Eventuate Tram (Java) | Cada paso de la saga se dispara por eventos: D5 publica `WalletDebited` ‚Üí D6 reacciona y procesa el pago ‚Üí si D6 publica `PaymentGatewayFailed`, D5 consume el evento y revierte el d√©bito autom√°ticamente. No hay coordinador central; cada servicio escucha y reacciona de forma independiente. Usa el mismo stack Node.js del dominio para no agregar complejidad tecnol√≥gica. | RNF-D5-02 |
| **Control de operaciones duplicadas** | **ElastiCache Redis** (guarda un identificador √∫nico por operaci√≥n durante 24 h) | Restricci√≥n `UNIQUE` directamente en Aurora | Si el usuario o la red reintenta una misma operaci√≥n, Redis la detecta y la rechaza antes de tocar la base de datos, evitando dobles d√©bitos. | RNF-D5-01, RNF-D5-02 |

---

### 4.12 Detalle de stack ‚Äî D6: Integraciones y Pasarelas de Pago

| Componente | Tecnolog√≠a | Alternativa | Por qu√© se eligi√≥ | RNF vinculado |
|---|---|---|---|---|
| **Registro de pasarelas (Adapter Registry)** | **NestJS con patr√≥n Strategy** (cada pasarela es un m√≥dulo independiente que se activa/desactiva en caliente) | Spring Cloud (Java) | Permite agregar o quitar una pasarela (por ejemplo, un nuevo proveedor de pagos) sin apagar el servicio. La configuraci√≥n de qu√© pasarelas est√°n activas se guarda en Aurora. | RNF-D6-02 |
| **Aislamiento de fallos por pasarela** | **`opossum`** (circuit breaker para Node.js), una instancia por cada pasarela | Resilience4j (Java) | Si PSE falla repetidamente, solo se corta el circuito de PSE; DRUO, ACH y Apple Pay siguen funcionando con normalidad. Es la librer√≠a de circuit breaker m√°s usada en el ecosistema Node.js. | RNF-D6-01 |
| **Pruebas de integraci√≥n sin depender de externos** | **OpenAPI 3.x** (contrato de cada pasarela) + **WireMock** (simula PSE, DRUO, ACH en el pipeline de CI) | Pact | Los tests de integraci√≥n corren en CI sin necesitar conexi√≥n real a PSE o ACH. El contrato OpenAPI detecta autom√°ticamente si una pasarela cambi√≥ su API. | RNF-D6-01, RNF-D6-02 |
| **Credenciales de cada pasarela** | **AWS Secrets Manager** (una clave por pasarela, rotaci√≥n autom√°tica) | HashiCorp Vault | Las claves de API de PSE, DRUO, ACH y Apple Pay se rotan autom√°ticamente sin reiniciar ning√∫n pod. Si se revoca una pasarela, sus credenciales no afectan a las dem√°s. | RNF-D6-01 |

---

### 4.13 Detalle de stack ‚Äî D7: Pagos Masivos a Empleados

| Componente | Tecnolog√≠a | Alternativa | Por qu√© se eligi√≥ | RNF vinculado |
|---|---|---|---|---|
| **Scheduler transaccional** | **ShedLock** (lock distribuido sobre Aurora PostgreSQL en AWS) + `@Scheduled` de Spring Boot | Quartz Scheduler | ShedLock garantiza que un job programado se ejecute exactamente una vez, incluso si hay m√∫ltiples r√©plicas del servicio: adquiere un lock en Aurora (AWS, D7 permanece completamente en AWS) antes de ejecutar y lo libera al terminar. M√°s ligero que Quartz; no requiere tablas adicionales complejas. | RNF-D7-04 |
| **Worker pool escalable** | **Spring Boot consumers** sobre Amazon MSK + **HPA** con m√©tricas de lag de Kafka (CloudWatch Adapter) | KEDA (Kubernetes Event-Driven Autoscaling) | Cada worker consume de una partici√≥n Kafka dedicada por empresa; HPA escala pods al detectar lag > umbral configurable. EC2 node groups permiten instancias de mayor capacidad en d√≠as de pago (14‚Äì16, 29‚Äì31). KEDA ofrece mayor granularidad pero con complejidad operacional adicional. | RNF-D7-01, RNF-D7-05 |
| **Aislamiento por empresa** | **Kafka topic particionado por `company_id`** + **Resilience4j** (circuit breaker por empresa) | Colas SQS separadas por empresa | Cada empresa tiene su propia partici√≥n (o conjunto de particiones); un fallo en la API de empresa A solo abre el circuito de A sin afectar a las dem√°s 14 empresas. Kafka mantiene orden y garant√≠a de entrega; SQS es alternativa m√°s simple pero con menor control sobre particionamiento y replay. | RNF-D7-05, RNF-D7-02 |
| **Idempotencia de pagos** | **`payroll_payment_id`** como idempotency key + constraint `UNIQUE` en Aurora + validaci√≥n previa en Redis (TTL 24 h) | Solo constraint en BD | Redis detecta duplicados antes de tocar Aurora, evitando contenci√≥n en la BD durante picos de 30K pagos. El constraint en Aurora act√∫a como red de seguridad final. | RNF-D7-02 |
| **Trazabilidad de lotes** | **`payroll_batch_id`** como correlation ID propagado en todos los eventos Kafka + registro en D8 | Logging manual | Cada pago individual hereda el `payroll_batch_id` de su lote padre, permitiendo consultar en D8/OpenSearch el estado completo de un lote y el detalle de cada pago. | RNF-D7-03 |

---

### ADR-001 ‚Äî Modelo h√≠brido adoptado (on-premise Colombia + AWS)

> **Estado:** Adoptado  
> **Fecha:** Febrero 2026  
> **Decisores:** Equipo de arquitectura + asesor√≠a legal  

#### Contexto

AWS no cuenta con regi√≥n en Colombia; la regi√≥n m√°s cercana es `sa-east-1` (S√£o Paulo, Brasil). La Superintendencia Financiera de Colombia confirm√≥ que el alojamiento de datos regulados (PII de usuarios, transacciones financieras, credenciales de autenticaci√≥n e hist√≥rico de auditor√≠a) fuera del territorio nacional **no es aceptable** bajo la Circular Externa 007 de 2018 y sus actualizaciones.

#### Decisi√≥n

Se adopta un **modelo h√≠brido** donde los datos sensibles y el c√≥mputo de los dominios m√°s cr√≠ticos residen en un datacenter on-premise en Colombia, conectado a AWS (`sa-east-1`) mediante AWS Direct Connect.

**On-premise (datacenter Colombia):**

| Componente | Dominio | Datos / C√≥mputo |
|---|---|---|
| **Keycloak 24 + NestJS API** | D1 ‚Äî IAM | C√≥mputo co-localizado con su BD |
| **PostgreSQL 16 (Patroni HA)** | D1 | Hashes de contrase√±as, tokens MFA, sesiones (~25M usuarios) |
| **PostgreSQL 16 (Patroni HA)** | D2 | PII: nombres, c√©dulas, cuentas bancarias (~25M usuarios) |
| **PostgreSQL 16 (Patroni HA)** | D4 | Montos, cuentas origen/destino, estados de transacci√≥n |
| **Apache Cassandra 4.1 (RF=3)** | D8 | Audit log inmutable, firmas digitales, hash chains (retenci√≥n ‚â• 5 a√±os) |
| **D8 Event Ingester** (Java/Spring Boot) | D8 | C√≥mputo co-localizado con Cassandra |
| **D8 Report Generator** (Python) | D8 | C√≥mputo co-localizado con Cassandra |
| **HashiCorp Vault** | Transversal | Secretos y cifrado de BDs on-premise |
| **Kubernetes 1.29** (kubeadm + MetalLB + Longhorn) | Infraestructura | Orquestaci√≥n de contenedores on-premise |

**AWS (`sa-east-1`):**

| Componente | Justificaci√≥n |
|---|---|
| **Amazon EKS** (D2, D3, D4, D5, D6, D7 services + D8 Dashboard API, Flink, Fraud List Manager) | C√≥mputo de servicios: procesan datos en tr√°nsito sin persistirlos localmente; D2 y D4 conectan a PostgreSQL on-premise v√≠a Direct Connect. |
| **Amazon MSK** | Eventos en tr√°nsito cifrados (TLS 1.3 + KMS); retenci√≥n temporal (7‚Äì14 d√≠as) antes de persistirse en Cassandra on-premise. |
| **ElastiCache Redis** | Datos ef√≠meros (cach√©, TTL corto); no constituyen almacenamiento permanente de PII. |
| **Aurora PostgreSQL** (D3, D5, D7) | Datos de menor sensibilidad regulatoria que pueden residir en AWS. |
| **Amazon OpenSearch Service** | Indexaci√≥n full-text del audit log para b√∫squedas operacionales (datos derivados, no fuente de verdad). |
| **Amazon S3** | Almacenamiento cifrado (SSE-KMS) de reportes regulatorios generados on-premise. |
| **CloudFront + Amplify** | CDN y frontend; no almacenan datos sensibles. |
| **CloudWatch, Managed Grafana, X-Ray** | M√©tricas operacionales; los logs con PII se enrutan al audit store on-premise. |
| **AWS Secrets Manager + KMS** | Secretos y cifrado de servicios en AWS (D3, D5, D6, D7, MSK, Redis). |

**Conectividad:**

| Aspecto | Detalle |
|---|---|
| **Enlace** | AWS Direct Connect dedicado, 2√ó 1 Gbps por rutas f√≠sicas distintas |
| **Latencia** | ~15‚Äì25 ms (Colombia ‚Üí S√£o Paulo) |
| **Cifrado** | VPN Site-to-Site sobre Direct Connect (IPSec) + TLS 1.3 de la aplicaci√≥n |
| **Costo** | ~$1,500‚Äì$3,000 USD/mes por puerto + transferencia de datos |

#### Consecuencias aceptadas

| Aspecto | Impacto | Mitigaci√≥n |
|---|---|---|
| **Latencia D2/D4 ‚Üí BD on-premise** | +15‚Äì25 ms por query SQL | Cach√© Redis en AWS (proxy de saldo TTL corto para D2; listas antifraude TTL 60 s y bancos filiales TTL 5 min para D4). Patr√≥n CQRS para lecturas frecuentes. |
| **Carga operacional on-premise** | Equipo de operaciones debe gestionar PostgreSQL, Cassandra, Vault, K8s, hardware | Patroni automatiza failover de PostgreSQL; Longhorn replica vol√∫menes; Terraform gestiona ambos entornos; runbooks documentados. |
| **Disponibilidad del datacenter** | Depende del proveedor colombiano | SLA contractual ‚â• 99.9% con generadores, UPS y redundancia de red. |
| **Disaster Recovery** | PostgreSQL on-premise es fuente de verdad | Streaming replication as√≠ncrona a r√©plica de lectura en AWS; promoci√≥n a primaria en caso de desastre del datacenter colombiano. RPO < 1 min, RTO < 30 min. |
| **Escalabilidad on-premise** | Escalar hardware es m√°s lento que escalar en AWS | Dimensionar para picos esperados (d√≠as de n√≥mina 14‚Äì16, 29‚Äì31); capacidad de reserva de 2√ó la carga pico. |

---


*Pr√≥xima secci√≥n: **Secci√≥n 5 ‚Äî Estrategias para facilitar la evoluci√≥n***
